# üß† Apprentissage machine - Notes de cours

## Introduction

#### Historique
- 1943: Apparition du neurone artificiel (McCullock & Pitts).
- 1957: Perceptron (Rosenblatt)

#### Pourquoi ?
* Environnements inconnus
* Agent qui s'am√©liore avec l'exp√©rience

#### Applications
* Classification
* R√©gression/pr√©diction
* Clustering d'√©l√©ments similaires
* Renforcement
* R√©duction de dimensionnalit√© 

#### Learning element 
* El√©ment de performance (agent logique, bas√© sur l'utilit√©, exploration, ...)
* Composant fonctionnel (fonction d'√©valuation, de perception/action, de transitions, ...)
* Repr√©sentation (fonction lin√©aire avec poids, axiomes, r√©seau neuronal, ...)
* Feedback (victoire/d√©faite, r√©sultat, ...)

#### Types d'apprentissage
* **Supervis√©** : Exemples d'entr√©es et sorties disponible, l'objectif est d'obtenir une fonction qui associe l'un √† l'autre
* **Semi-supervis√©** : Les donn√©es d'apprentissage n'ont pas toujours une √©tiquette
* **Non-supervis√©** : Extraction des patterns dans les entr√©es sans direction sp√©cifique sans aucun feedback. Les r√©sultats sont √† interpr√©ter.
* **Renforcement** : Apprend en fonction des r√©compenses/punitions suite √† ses actions.

#### Repr√©sentation des donn√©es
S√©quence de valeurs r√©elles/ent√®res, matrices, vecteurs one-hot, bag-of-words ou tf-idf, ...

#### Flow typique du ML
1. **Preprocessing du dataset** : Feature extraction/scaling/selection, r√©duction de dimensionnalit√©, √©chantillonage, ...
2. **Apprentissage** : S√©lection du mod√®le, cross-validation, ...
3. **Evaluation** : M√©triques, optimisation des hyper-param√®tres, ...
4. **Pr√©diction**

## Vocabulaire et concepts

##### Epoch
Un passage complet sur l'ensemble de donn√©es.

##### Batch
Un sous-ensemble des donn√©es d'entra√Ænement lorsque celui-ci est trop volumineux pour √™tre utilis√© enti√®rement √† chaque fois.

Dans ce cas, on ajoute la notion d'*it√©ration* qui est le nombre de batches n√©cessaire pour compl√©ter une epoch

##### Feature scaling 
Normalise les donn√©es suivant une √©chelle (min-max, z-score, ...) pour √©viter les probl√®mes avec les mod√®les qui utilisent la notion de distance

##### Data imbalance 
Ph√©nom√®ne qui se produit lorsque la proportion d'exemples de chaque classe est tr√®s diff√©rente, ce qui tend √† favoriser les classes avec un grand nombre d'√©chantillons

##### Sous-apprentissage et sur-apprentissage
<img src="imgs/overfit.png" width="500">

##### Compromis biais-variance
Un haut biais permet un apprentissage g√©n√©ralement plus rapide et simple, mais procure de moins bonne performances sur les probl√®mes complexes (e.g. logistic regression, linear regression, ...).

Une haute variance permet d'impacter l'estimation de la fonction cible est d'√™tre plus r√©ceptif aux particularit√©s des donn√©es, ce qui est d√©sirable mais dans une certaine mesure afin d'√©viter l'overfitting (e.g. arbres de d√©cisions, knn, svm, ...).

C'est donc un compromis √† faire. On peut utiliser par exemple la r√©gularisation L2 (qui introduit un nouveau biais qui p√©naliser les valeurs extr√™mes de poids, √† condition d'avoir fait du feature scaling).


##### Param√©trique et non-param√®trique
**Param√©trique** :
Estimation des param√®tres √† partir des donn√©es d'entra√Ænement. Le nombre de param√®tres est ind√©pendant du nombre de donn√©es (e.g. perceptron, SVM lin√©aire, logistic regression, ...).

L'avantage des mod√®les param√®triques est qu'ils sont g√©n√©ralement plus simple, l'apprentissage est rapide. ?√©anmoins ils sont souvent limit√©s aux probl√®mes simples.

**Non-param√©trique** :
Le nombre de param√®tres augmente avec la taille des donn√©es (e.g. DT, SVM, KNN, ...).

L'avantage des ces mod√®les est qu'ils sont g√©n√©ralement plus flexibles et puissant, mais ils sont plus lent √† entrainer et n√©cessite plus de donn√©es d'entrainement. Le risque de sur-apprentissage est √©galement plus √©lev√©.

G√©n√©ralement les m√©thodes non-param√©triques sont utiles lorsque l'on a beaucoup de donn√©es et aucune connaissance au pr√©alable, et qu'on veut √©viter de choisir les features.

## Classification simple

### Perceptron
<img src="imgs/perceptron_schema.png" width="500">

Le perceptron apprend les poids optimaux √† multiplier avec les entr√©es pour d√©terminer si le neurone s'active ou non. La fonction d'activation est celle de Heaviside (+1 si positif, -1 si n√©gatif). Il s'agit donc d'un outil de classification binaire.

<img src="imgs/perceptron.png" width="300">

A noter que w<sub>0</sub>x<sub>0</sub> = œë est appel√© le biais.

Le principe est le suivant : 
1. Les poids sont initalis√©s √† ‚âà 0
2. Pour chaque exemple d'entra√Ænement x<sup>(i)</sup> :
  1. On calcule la sortie estim√©e y<sup>(i)</sup>
  2. On met √† jour les poids w<sub>j</sub> += Œî w<sub>j</sub>

<img src="imgs/perceptron_weight.png" width="200">

*Œ∑* est un nombre entre 0 et 1 qui constitue le learning rate.

A noter que Œî w<sub>j</sub> est nul si la pr√©diction est correcte, donc le poids ne changera pas.

#### Remarques
Si le dataset n'est pas s√©parable lin√©airement, le perceptron bouclera √† l'infini.

Il est possible d'√©tendre le perceptron pour faire de la classification multi-classe par du *One-vs-All*, qui consiste √† cr√©er un classeur sp√©cialis√© dans la d√©tection d'une classe en particulier, puis de d√©terminer quel classeur est actif pour tel entr√©e.

### Adaptive linear neurons (Adaline)
<img src="imgs/adaline_schema.png" width="500">

Sur le m√™me principe que le perceptron, toutefois la fonction d'activation est lin√©raire, ce qui permet d'avoir des sorties continues plut√¥t que binaire. On rajoute parfois un *quantizer* pour la pr√©diction de classe.

On charche √† optimiser la fonction de co√ªts *Sum of Squared Errors* (SSE) entre les sorties et les vraies classes :

<img src="imgs/sse.png" width="200">

On utilise pour cela l'algorithme du gradient, notamment parce que J(w) est convexe mais aussi parce que c'est tr√®s rapide.

<img src="imgs/gradient.png" width="200">

Toutefois, on utilise g√©n√©ralement la *descente de gradient stochastique* (SGD) pour √©viter l'utilisation du dataset complet et acc√©lerer le temps de calcul. Dans ce cas, il faut veiller √† m√©langer le dataset pour obtenir des r√©sultats satisfaisants. Un learning rate adaptatif est particuli√®rement adapt√© avec le SGD. 

<img src="imgs/sgd.png" width="200">

#### Remarques
La mise √† jour des poids se fait sur le dataset en entier, √† l'inverse du perceptron o√π elle se fait exemple par exemple.

Le feature scaling permet d'acc√©lerer la descente du gradient.

Une valeur de learning rate *Œ∑* forte peut emp√™cher de converger (si le pas du gradient est trop √©lev√©) tandis qu'une valeur faible augmentera consid√©rablement le temps de calcul.

## Classification

### Logistic regression
<img src="imgs/logistic_schema.png" width="500">

La fonction d'activation est une sigmo√Øde (fonction logistique). Elle se base sur le principe du rapport des chances (odds ratio), c'est-√†-dire qu'une √©chantillon appartienne √† une certaine classe √©tant donn√© ses attributs.

La fonction de co√ªts est modifi√©e pour utilis√©e les logarithmes (Log-likelyhood) afin d'√©viter les underflow, en plus de trasformer les produits en somme, ce qui en soit les calculs plus faciles.

#### Remarques
La logistic regression est plus sensible aux outliers. Il s'agit √©galement d'un mod√®le simple qui est facile √† mettre en place et √† jour.

### Support vector machine (SVM)
<img src="imgs/svm_schema.png" width="500">

Le principe est de maximiser les marges (distance entre un hyperplan et les √©chantillons les plus proches de ce plan, a.k.a support vectors) entre les classes, dans le but d'√©viter l'overfitting et permettre une meilleure g√©n√©ralisation.

Un avantage du SVM est de pouvoir faire de la kernalisation. Il s'agit d'utiliser un noyau qui exploite des combinaisons non-lin√©aires et des attrinuts originaux en les projetant dans un espace √† plus haute dimension, dans laquelle ils deviennent potentiellement s√©parables (e.g. radial basis function kernel).

![Kernel](imgs/kernel.png)

#### Remarques
Les SVMs se concentrent principalement sur les √©chantillons √† la fronti√®re des classes. En pratique, les r√©sultats sont similaires ) la logistic regression.

Si on utilise un kernel, il est n√©cessaire d'entra√Æner le SVM dans la dimension sup√©rieure (logique) : il faut donc transformer toutes les entr√©es auparavant.

Le *gamma* du RBF kernel d√©termine l'influence des √©chantillons.

### Arbres de d√©cision
<img src="imgs/decision_tree_schema.png" width="500">

Le principe est d'exploiter les attributs de l'ensemble de donn√©es pour apprendre une s√©rie de "questions" pour inf√©rer les classes. Chaque noeud s√©pare les donn√©es qui permettent d'obtenir le plus grand gain d'information, et ce processus est r√©p√©t√© jusqu'√† ce que les feuilles soient *pures* (i.e. ne repr√©sentent qu'une seule classe).

On teste sur le dataset du parent *D<sub>p</sub>* la s√©paration selon l'attribut *f*, en fonction de l'impuret√© *I* et du nombre d'√©chantillons *N* du parent et des enfants :

<img src="imgs/info_gain.png" width="300">

L'impuret√© (qu'on cherche √† r√©duire), est calcul√©e selon l'entropie (elle-m√™me calcul√©e sur la proportion d'√©chantillons appartenant √† une certaine classe pour un certain noeud). C'est-√†-dire qu'elle sera nul si tous les √©chantillons sont dans la m√™me classe et maximale si tous les √©chantillons sont diff√©rents.

L'indice gini peut aussi √™tre utilis√© pour le calcul de l'impuret√©. Celui-ci est similaire √† l'entropie mais se base sur la probabilit√© d'erreur de classification.

#### Remarques
G√©n√©ralement, on fait une s√©paration binaire pour gagner en temps de calcul.

### Random forest
Il s'agit d'un ensemble d'arbres de d√©cisions (estimators), dont la classification se fait au vote majoritaire sur les k arbres.

Les performances sont g√©n√©ralement meilleures qu'avec un seul arbre.

### K-nearest neighbors (KNN)
<img src="imgs/knn.png" width="300">

Le principe est de se dire que les √©l√©ments proches les uns par rapport aux autres sont probablement de la m√™me classe.

L'algorithme est le suivant :
1. Choisis un nombre de voisins *k* et une mesure de distance
2. Trouver les *k* voisins les plus proches √† classer
3. Vote √† la majorit√© (pond√©r√©e ou non)

On utilise g√©n√©ralement une distance de minkowski.  

Un inconv√©nient majeur de cette m√©thode est que l'ensemble des exemples d'entrainement doit √™tre gard√© en m√©moire, et par cons√©quent les knn sont peu performants pour une dimensionnalit√© √©lev√©e.

## Ensemble learning
<img src="imgs/ensemble_learning.png" width="400">

On part du principe qu'un ensemble de classeur plus faibles performent mieux qu'un seul tr√®s bon classeur.

A partir d'un ensemble d'entra√Ænement, un certain nombre de classeurs sont produits (ceux-ci peuvent potentiellement √™tre diff√©rents algorithmes ou sous-ensemble d'entra√Ænement).

### Bagging (bootstrap aggregation)
<img src="imgs/bootstrap.png" width="400">

Le principe est d'utiliser diff√©rents sous-ensemble d'apprentissage (bootstrap) al√©atoire avec remise.

Chaque bootstrap est utilis√© pour entra√Æner un classeur.

Cela permet de r√©duire la variance, mais pas le biais. C'est pour cette raison qu'on utilise g√©n√©ralement des classeurs avec un biais faible (e.g. decision trees).

### Boosting
Le principe est de se focaliser sur les exemples difficiles √† classer.

L'algorithme est le suivant :
1. Tirer un sous-ensemble du dataset pour entrainer un premier classeur
2. Tirer un autre sous-ensemble du dataset et y rajouter 50% des exemples mal class√©s par le classeur pr√©c√©dent pour entrainer le suivant
3. R√©p√©ter l'op√©ration autant de fois que l'on veut
4. Faire un vote majoritaire

Th√©oriquement, cela permet de r√©duire √† la fois le biais et la variance contrairement au bagging. Toutefois, la variance est souvent √©lev√©e dans les algorithmes de boosting.

#### Adaboost
Il s'agit d'un mod√®le de boosting qui utilise le dataset complet et dont les √©chantillons sont pond√©r√©s √† chaque √©tape. Cela permet de construire un classeur qui apprend des erreurs pass√©es.

![Adaboost](imgs/adaboost.png)

Exemple : 
1. Maximisation de la fonction de co√ªts
2. Ajustement des poids, et entrainement d'un second classeur
3. Ajustement des poids, et entrainement d'un troisi√®me classeur
4. Vote majoritaire
